{
	"name": "000 Retail-scoring-onnx-HB-AML_Ignite",
	"properties": {
		"folder": {
			"name": "Demo notebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "hummingbird",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2
		},
		"metadata": {
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7e416de3-c506-4776-8270-83fd73c6cc37/resourceGroups/demosynapserg/providers/Microsoft.Synapse/workspaces/wsazuresynapseanalytics/bigDataPools/hummingbird",
				"name": "hummingbird",
				"type": "Spark",
				"endpoint": "https://wsazuresynapseanalytics.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/hummingbird",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56
			}
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Near real-time sales forecasting leveraging Synapse Link for Azure Cosmos DB\n",
					"\n",
					"Microsoft Retail Store has built its new-age supply chain management system on Azure Cosmos DB.\n",
					"\n",
					"The supply chain management system tracks retail operations across 1000s of locations across the world and tracks inventory across the 100s of Microsoft product SKUs sold.\n",
					"\n",
					"This notebook shows the power of Synapse Link for Cosmos DB to be able to run near real-time analytics over operational data, without ETL and without impact to transactional workloads.\n",
					"\n",
					"In particular, the **goal here is to build a sales forecasting model to help store locations customize their inventory planning in real-time based on flucutations in demand.**\n",
					"\n",
					"<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/store.PNG\" alt=\"Surface Device\" width=\"75%\"/>\n",
					"\n",
					"&nbsp;\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Pre-requisites run this Notebook \n",
					"No action required in this WS. Just use Spark pool: hummingbird\n",
					"\n",
					"\n",
					"### requirements.txt:\n",
					"* hummingbird-ml==0.0.6\n",
					"* skl2onnx==1.7.0\n",
					"* mlflow==1.10.0\n",
					"* azureml-mlflow==1.12.0\n",
					"\n",
					"\n",
					"\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Import scikit-learn's RandomForestClassifier, ONNX, and Hummingbird\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import time\n",
					"import numpy as np\n",
					"import pandas as pd\n",
					"import onnxruntime as ort\n",
					"import hummingbird.ml\n",
					"\n",
					"from datetime import datetime\n",
					"from timeit import default_timer as timer\n",
					"from sklearn.ensemble import RandomForestRegressor\n",
					"from sklearn.model_selection import train_test_split\n",
					"from sklearn.pipeline import Pipeline\n",
					"from sklearn.preprocessing import StandardScaler\n",
					"from sklearn.compose import ColumnTransformer\n",
					"from skl2onnx import convert_sklearn\n",
					"from skl2onnx.common.data_types import DoubleTensorType"
				],
				"attachments": null,
				"execution_count": 53
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Load data from Cosmos DB and convert to Pandas \n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"_rid"
							],
							"values": [
								"_ts"
							],
							"yLabel": "_ts",
							"xLabel": "_rid",
							"aggregation": "SUM",
							"aggByBackend": false
						},
						"aggData": "{\"_ts\":{\"sY0TAIh7N8gBAAAAAAAABA==\":1598476494,\"sY0TAIh7N8gCAAAAAAAABA==\":1598476494,\"sY0TAIh7N8gDAAAAAAAABA==\":1598476494}}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					}
				},
				"source": [
					"# Load and join data\n",
					"df_StoreDemographics = spark.read\\\n",
					"                        .format(\"cosmos.olap\")\\\n",
					"                        .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
					"                        .option(\"spark.cosmos.container\", \"StoreDemoGraphics\")\\\n",
					"                        .load()\\\n",
					"                        .toPandas()\n",
					"\n",
					"df_RetailSales = spark.read\\\n",
					"                        .format(\"cosmos.olap\")\\\n",
					"                        .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
					"                        .option(\"spark.cosmos.container\", \"RetailSales\")\\\n",
					"                        .load()\\\n",
					"                        .toPandas()\n",
					"\n",
					"df_Products = spark.read\\\n",
					"                    .format(\"cosmos.olap\")\\\n",
					"                    .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
					"                    .option(\"spark.cosmos.container\", \"Products\")\\\n",
					"                    .load()\\\n",
					"                    .toPandas()\n",
					"\n",
					"display(df_Products.head(10))\n",
					"\n",
					"df = df_RetailSales.merge(df_Products, on=['productCode'], how='left').merge(df_StoreDemographics, on=['storeId'], how = 'left')\n",
					"df.head(5)"
				],
				"attachments": null,
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Define some featurizers: dictionary encoder and date featurizer\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def dictionary_encode(df, column, dictionary):\n",
					"\n",
					"    def add_into_dict(value, dictionary):\n",
					"        count = 0\n",
					"        if len(dictionary) > 0:\n",
					"            count = max(dictionary.values())\n",
					"        count += 1\n",
					"        dictionary[value] = count\n",
					"        return count\n",
					"\n",
					"    df[column] = pd.DataFrame(df[column].values)[0].apply(lambda x: dictionary[x] if x in dictionary else add_into_dict(x, dictionary))\n",
					"\n",
					"def date_featurize(df,column):\n",
					"    df[column] = pd.DataFrame(df[column].values)[0].apply(lambda x: (int(datetime.strptime(x, '%m/%d/%Y').strftime('%m%d%Y'))))"
				],
				"attachments": null,
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Featurize the data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dictionary = {}\n",
					"\n",
					"dictionary_encode(df,'productCode', dictionary)\n",
					"date_featurize(df,'weekStarting')\n",
					"df.head(5)"
				],
				"attachments": null,
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"# Drop unecessary columns\n",
					"df = df.drop(columns=['logQuantity', '_rid_x', '_rid_y', 'id','_rid','_ts','_ts_x','_ts_y','_etag','_etag_x','_etag_y','id_x','id_y'])\n",
					"df.head(5)\n",
					"\n",
					"#print(df.iloc[0])"
				],
				"attachments": null,
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"source": [
					"#### Now that we selected the right columns, store the dataframe for querying with PREDICT later\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"spark_df = raven_session.createDataFrame(df)\n",
					"spark_df.createTempView(\"combined\")"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"source": [
					"#store columns\n",
					"features = np.array(df.drop('quantity', axis = 1))\n",
					"\n",
					"train_df, test_df = train_test_split(df, test_size=0.2)\n",
					"\n",
					"train_features = pd.DataFrame(train_df.drop(['quantity'], axis = 1))\n",
					"train_labels = pd.DataFrame(train_df.iloc[:,train_df.columns.tolist().index('quantity')])\n",
					"\n",
					"test_features = pd.DataFrame(test_df.drop(['quantity'], axis = 1))\n",
					"\n",
					"test_labels = pd.DataFrame(test_df.iloc[:,test_df.columns.tolist().index('quantity')])\n",
					"\n",
					"# Convert test_features from df to numpy array\n",
					"#test_features = np.array(test_features) #if data comes from ADLSgen2\n",
					"test_features = np.array(test_features).astype(np.float64) #if data comes from Cosmos DB\n",
					"\n",
					"# Split the test data for hummingbird\n",
					"test_data = tuple(np.split(test_features, 18, axis=1))\n",
					"\n",
					""
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"source": [
					"# Define the pipeline\n",
					"scaler_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
					"preprocessor = ColumnTransformer(transformers=[('scaling', scaler_transformer, list(train_features.columns))])\n",
					"model = RandomForestRegressor(n_estimators=1000, max_depth=9)\n",
					"pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])"
				],
				"attachments": null,
				"execution_count": 14
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Train a Random Forest Regressor model\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"pipeline.fit(train_features, train_labels)"
				],
				"attachments": null,
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"source": [
					"def input_types(features):\n",
					"    inputs = []\n",
					"    for columnName, columnType in zip(features.columns, features.dtypes):\n",
					"        inputs.append((str(columnName), DoubleTensorType([None, 1])))\n",
					"    return inputs\n",
					"\n",
					"# For ONNX input:\n",
					"initial_types = input_types(train_features)\n",
					"# For HB input:\n",
					"columns =[x for (x,y) in initial_types]"
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Convert the trained model into ONNX using skl2onnx\n",
					"\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"onnx_ml_model = convert_sklearn(pipeline, initial_types=initial_types, final_types=[('quantity', DoubleTensorType([None, 1]))])"
				],
				"attachments": null,
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"sess = ort.InferenceSession(onnx_ml_model.SerializeToString())\n",
					"inputs = sess.get_inputs()\n",
					"outputs = sess.get_outputs()\n",
					"\n",
					"test_inputs = { inputs[i].name: test_features[:,i].reshape(-1,1) for i in range(len(inputs)) }\n",
					"target_name = outputs[0].name;"
				],
				"attachments": null,
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"source": [
					"## ONNX (traditional) model\n",
					"\n",
					"<img src=\"https://sqlchoice.blob.core.windows.net/sqlchoice/static/images/retail_skl2onnx_onnx.png\" alt=\"Surface Device\" width=\"80%\"/>"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Optimize and convert the trained model to ONNX with Hummingbird\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Hummingbird\n",
					"#### https://github.com/microsoft/hummingbird\n",
					"Hummingbird is a library for compiling trained traditional ML models into tensor computations. Hummingbird allows users to seamlessly leverage neural network frameworks (such as PyTorch) to accelerate traditional ML models.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from hummingbird.ml import constants\n",
					"test_data = tuple(np.split(test_features, 18, axis=1))\n",
					"extra_config = {constants.INPUT_NAMES: columns, constants.OUTPUT_NAMES: [\"quantity\"]}\n",
					"hb_model = hummingbird.ml.convert(pipeline, \"onnx\", test_data, extra_config=extra_config)"
				],
				"attachments": null,
				"execution_count": 23
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Hummingbird model\n",
					"<img src=\"https://sqlchoice.blob.core.windows.net/sqlchoice/static/images/retail_hb.png\" alt=\"Surface Device\" width=\"80%\"/>\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Get Azure ML workspace context\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"outputCollapsed": true
				},
				"source": [
					"from azureml.core import Workspace\n",
					"\n",
					"subscription_id = \"58f8824d-32b0-4825-9825-02fa6a801546\" #you should be owner or contributor\n",
					"resource_group = \"prlangadrg\" #you should be owner or contributor\n",
					"workspace_name = \"amlwsdemos\" #your workspace name\n",
					"\n",
					"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
					"\n",
					"print('Workspace name: ' + ws.name, \n",
					"      'Azure region: ' + ws.location, \n",
					"      'Subscription id: ' + ws.subscription_id, \n",
					"      'Resource group: ' + ws.resource_group, sep = '\\n')"
				],
				"attachments": null,
				"execution_count": 29
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Log the model in Azure ML model registry with MLFlow\n",
					"Capturing model inputs/outputs in the signature\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import mlflow\n",
					"import mlflow.onnx\n",
					"\n",
					"from mlflow.models.signature import infer_signature\n",
					"\n",
					"experiment_name = 'synapse_retail_model_hb'\n",
					"artifact_path = 'synapse_retail_model_hb_artifact'\n",
					"\n",
					"mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
					"mlflow.set_experiment(experiment_name)\n",
					"\n",
					"with mlflow.start_run() as run:\n",
					"    # Infer signature\n",
					"    input_sample = train_features.head(1).astype(np.float64)\n",
					"    output_sample = pd.DataFrame(columns=['quantity'], data=[1234.0]).astype(np.float64)\n",
					"    signature = infer_signature(input_sample, output_sample)\n",
					"\n",
					"    # Save the model to the outputs directory for capture\n",
					"    mlflow.onnx.log_model(hb_model.model, artifact_path, signature=signature)\n",
					"\n",
					"    # Register the model to AML model registry\n",
					"    mlflow.register_model('runs:/' + run.info.run_id + '/' + artifact_path, 'synapse_retail_model_hb')"
				],
				"execution_count": 31
			}
		]
	}
}