{
	"name": "03-Retail-scoring-onnx-HB-Raven-AML_Live",
	"properties": {
		"folder": {
			"name": "Test notebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "hummingbird",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2
		},
		"metadata": {
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7e416de3-c506-4776-8270-83fd73c6cc37/resourceGroups/demosynapserg/providers/Microsoft.Synapse/workspaces/wsazuresynapseanalytics/bigDataPools/hummingbird",
				"name": "hummingbird",
				"type": "Spark",
				"endpoint": "https://wsazuresynapseanalytics.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/hummingbird",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56
			}
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Near real-time sales forecasting leveraging Synapse Link for Azure Cosmos DB\n",
					"\n",
					"Microsoft Retail Store has built its new-age supply chain management system on Azure Cosmos DB.\n",
					"\n",
					"The supply chain management system tracks retail operations across 1000s of locations across the world and tracks inventory across the 100s of Microsoft product SKUs sold.\n",
					"\n",
					"This notebook shows the power of Synapse Link for Cosmos DB to be able to run near real-time analytics over operational data, without ETL and without impact to transactional workloads.\n",
					"\n",
					"In particular, the **goal here is to build a sales forecasting model to help store locations customize their inventory planning in real-time based on flucutations in demand.**\n",
					"\n",
					"<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/store.PNG\" alt=\"Surface Device\" width=\"75%\"/>\n",
					"\n",
					"&nbsp;\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Pre-requisites run this Notebook \n",
					"No action required in this WS. Just use Spark pool: hummingbird\n",
					"\n",
					"### Jar file\n",
					"* [Jar file](https://microsoft-my.sharepoint.com/:u:/p/negust/ETzdAH0O3qREpCQ3gVslnEUB4ToF6whxTuf1ovhGbLVMPw?e=M684hb) needs to be placed in default storage account\n",
					"### requirements.txt:\n",
					"* hummingbird-ml==0.0.6\n",
					"* skl2onnx==1.7.0\n",
					"* azure-storage-blob==12.5.0\n",
					"* mlflow==1.10.0\n",
					"* azureml-mlflow==1.12.0\n",
					"\n",
					"\n",
					"### Also: [.whl](https://files.pythonhosted.org/packages/a3/b2/15e067fab7d98afc4f8f9f2a86af52508b8d0397fa1bfbf3fca79173d8f7/pyarrow-0.14.1-cp36-cp36m-manylinux1_x86_64.whl) for pyarrow == 0.14.1 because of [bug](https://github.com/astrolabsoftware/fink-broker/issues/284)\n",
					" * [installation instructions for .whl in synapse](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-azure-portal-add-libraries#install-a-custom-wheel-file)\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Import Raven jars and start the Raven Session\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%configure -f\n",
					"{\n",
					"    \"conf\": {\n",
					"        \"spark.jars\": \"abfss://default@azuresynapsesa.dfs.core.windows.net/artifacts/raven_2.11-1.0-2.4.5-fixed.jar\",\n",
					"        \"spark.submit.pyFiles\": \"abfss://default@azuresynapsesa.dfs.core.windows.net/artifacts/raven_2.11-1.0-2.4.5-fixed.jar\"\n",
					"    }\n",
					"}"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from raven.session import RavenSession\n",
					"\n",
					"raven_session = RavenSession.builder.appName(\"RavenSess\").getOrCreate()"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Import scikit-learn's RandomForestClassifier, ONNX, and Hummingbird\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import time\n",
					"import numpy as np\n",
					"import pandas as pd\n",
					"import onnxruntime as ort\n",
					"import hummingbird.ml\n",
					"\n",
					"from datetime import datetime\n",
					"from timeit import default_timer as timer\n",
					"from sklearn.ensemble import RandomForestRegressor\n",
					"from sklearn.model_selection import train_test_split\n",
					"from sklearn.pipeline import Pipeline\n",
					"from sklearn.preprocessing import StandardScaler\n",
					"from sklearn.compose import ColumnTransformer\n",
					"from skl2onnx import convert_sklearn\n",
					"from skl2onnx.common.data_types import DoubleTensorType"
				],
				"attachments": null,
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Load data from Cosmos DB and convert to Pandas \n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"_rid"
							],
							"values": [
								"_ts"
							],
							"yLabel": "_ts",
							"xLabel": "_rid",
							"aggregation": "SUM",
							"aggByBackend": false
						},
						"aggData": "{\"_ts\":{\"sY0TAIh7N8gBAAAAAAAABA==\":1598476494,\"sY0TAIh7N8gCAAAAAAAABA==\":1598476494,\"sY0TAIh7N8gDAAAAAAAABA==\":1598476494}}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					}
				},
				"source": [
					"# Load and join data\n",
					"df_StoreDemographics = spark.read\\\n",
					"                        .format(\"cosmos.olap\")\\\n",
					"                        .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
					"                        .option(\"spark.cosmos.container\", \"StoreDemoGraphics\")\\\n",
					"                        .load()\\\n",
					"                        .toPandas()\n",
					"\n",
					"df_RetailSales = spark.read\\\n",
					"                        .format(\"cosmos.olap\")\\\n",
					"                        .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
					"                        .option(\"spark.cosmos.container\", \"RetailSales\")\\\n",
					"                        .load()\\\n",
					"                        .toPandas()\n",
					"\n",
					"df_Products = spark.read\\\n",
					"                    .format(\"cosmos.olap\")\\\n",
					"                    .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
					"                    .option(\"spark.cosmos.container\", \"Products\")\\\n",
					"                    .load()\\\n",
					"                    .toPandas()\n",
					"\n",
					"display(df_Products.head(10))\n",
					"\n",
					"df = df_RetailSales.merge(df_Products, on=['productCode'], how='left').merge(df_StoreDemographics, on=['storeId'], how = 'left')\n",
					"df.head(5)"
				],
				"attachments": null,
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"source": [
					"### Define some featurizers: dictionary encoder and date featurizer\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def dictionary_encode(df, column, dictionary):\n",
					"\n",
					"    def add_into_dict(value, dictionary):\n",
					"        count = 0\n",
					"        if len(dictionary) > 0:\n",
					"            count = max(dictionary.values())\n",
					"        count += 1\n",
					"        dictionary[value] = count\n",
					"        return count\n",
					"\n",
					"    df[column] = pd.DataFrame(df[column].values)[0].apply(lambda x: dictionary[x] if x in dictionary else add_into_dict(x, dictionary))\n",
					"\n",
					"def date_featurize(df,column):\n",
					"    df[column] = pd.DataFrame(df[column].values)[0].apply(lambda x: (int(datetime.strptime(x, '%m/%d/%Y').strftime('%m%d%Y'))))"
				],
				"attachments": null,
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Featurize the data"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dictionary = {}\n",
					"\n",
					"dictionary_encode(df,'productCode', dictionary)\n",
					"date_featurize(df,'weekStarting')\n",
					"df.head(5)"
				],
				"attachments": null,
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"# Drop unecessary columns\n",
					"df = df.drop(columns=['logQuantity', '_rid_x', '_rid_y', 'id','_rid','_ts','_ts_x','_ts_y','_etag','_etag_x','_etag_y','id_x','id_y'])\n",
					"df.head(5)\n",
					"\n",
					"#print(df.iloc[0])"
				],
				"attachments": null,
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"source": [
					"#### Now that we selected the right columns, store the dataframe for querying with PREDICT later\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"spark_df = raven_session.createDataFrame(df)\n",
					"spark_df.createTempView(\"combined\")"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					"#store columns\n",
					"features = np.array(df.drop('quantity', axis = 1))\n",
					"\n",
					"train_df, test_df = train_test_split(df, test_size=0.2)\n",
					"\n",
					"train_features = pd.DataFrame(train_df.drop(['quantity'], axis = 1))\n",
					"train_labels = pd.DataFrame(train_df.iloc[:,train_df.columns.tolist().index('quantity')])\n",
					"\n",
					"test_features = pd.DataFrame(test_df.drop(['quantity'], axis = 1))\n",
					"\n",
					"test_labels = pd.DataFrame(test_df.iloc[:,test_df.columns.tolist().index('quantity')])\n",
					"\n",
					"# Convert test_features from df to numpy array\n",
					"#test_features = np.array(test_features) #if data comes from ADLSgen2\n",
					"test_features = np.array(test_features).astype(np.float64) #if data comes from Cosmos DB\n",
					"\n",
					"# Split the test data for hummingbird\n",
					"test_data = tuple(np.split(test_features, 18, axis=1))\n",
					"\n",
					""
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"# Define the pipeline\n",
					"scaler_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
					"preprocessor = ColumnTransformer(transformers=[('scaling', scaler_transformer, list(train_features.columns))])\n",
					"model = RandomForestRegressor(n_estimators=1000, max_depth=9)\n",
					"pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])"
				],
				"attachments": null,
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Train a Random Forest Regressor model\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"pipeline.fit(train_features, train_labels)"
				],
				"attachments": null,
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"def input_types(features):\n",
					"    inputs = []\n",
					"    for columnName, columnType in zip(features.columns, features.dtypes):\n",
					"        inputs.append((str(columnName), DoubleTensorType([None, 1])))\n",
					"    return inputs\n",
					"\n",
					"# For ONNX input:\n",
					"initial_types = input_types(train_features)\n",
					"# For HB input:\n",
					"columns =[x for (x,y) in initial_types]"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Convert the trained model into ONNX using skl2onnx\n",
					"\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"onnx_ml_model = convert_sklearn(pipeline, initial_types=initial_types, final_types=[('quantity', DoubleTensorType([None, 1]))])"
				],
				"attachments": null,
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"sess = ort.InferenceSession(onnx_ml_model.SerializeToString())\n",
					"inputs = sess.get_inputs()\n",
					"outputs = sess.get_outputs()\n",
					"\n",
					"test_inputs = { inputs[i].name: test_features[:,i].reshape(-1,1) for i in range(len(inputs)) }\n",
					"target_name = outputs[0].name;"
				],
				"attachments": null,
				"execution_count": 14
			},
			{
				"cell_type": "markdown",
				"source": [
					"## ONNX (traditional) model\n",
					"\n",
					"<img src=\"https://sqlchoice.blob.core.windows.net/sqlchoice/static/images/retail_skl2onnx_onnx.png\" alt=\"Surface Device\" width=\"80%\"/>"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"# Hummingbird\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"\n",
					"\n",
					"<img src=\"https://sqlchoice.blob.core.windows.net/sqlchoice/static/images/HB_slide.png\" alt=\"Surface Device\" width=\"90%\"/>\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Optimize and convert the trained model with Hummingbird\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## https://github.com/microsoft/hummingbird\n",
					"Hummingbird is a library for compiling trained traditional ML models into tensor computations. Hummingbird allows users to seamlessly leverage neural network frameworks (such as PyTorch) to accelerate traditional ML models.\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"# Model scoring speedups with Hummingbird\n",
					"\n",
					"\n",
					"<img src=\"https://sqlchoice.blob.core.windows.net/sqlchoice/static/images/chart-hb.png\" alt=\"Surface Device\" width=\"60%\"/>\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from hummingbird.ml import constants\n",
					"test_data = tuple(np.split(test_features, 18, axis=1))\n",
					"extra_config = {constants.INPUT_NAMES: columns, constants.OUTPUT_NAMES: [\"quantity\"]}\n",
					"hb_model = hummingbird.ml.convert(pipeline, \"onnx\", test_data, extra_config=extra_config)"
				],
				"attachments": null,
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Hummingbird model\n",
					"<img src=\"https://sqlchoice.blob.core.windows.net/sqlchoice/static/images/retail_hb.png\" alt=\"Surface Device\" width=\"80%\"/>\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"#### Save the ONNX model to ABFS and the Spark Context\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"inputCollapsed": true
				},
				"source": [
					"from azure.storage.blob import BlobServiceClient\n",
					"from azure.storage.blob import BlobClient\n",
					"#\n",
					"\n",
					"# first write to Azure Storage. Setup credentials\n",
					"#connection_string = \"DefaultEndpointsProtocol=https;AccountName=humingbird;AccountKey=WDOwhWhgi1cfUrQkzKpDnTq8JabnhemjGLm7ToMHGrP0XrjGfg/WKRtQ58aTsJ8WGRpUZi8WWghWOm+q2YkrSA==;EndpointSuffix=core.windows.net\"\n",
					"connection_string = \"DefaultEndpointsProtocol=https;AccountName=negustdatalake;AccountKey=zuTO6o2r+8f3fppNu5hHAIcjmltKk76ayMPtIPhLxQ0njllTYrFyAVeUj7ytyVHmI8C+1jlg1JrVkb+kLgi3dQ==;EndpointSuffix=core.windows.net\"\n",
					"service = BlobServiceClient.from_connection_string(conn_str=connection_string)\n",
					"\n",
					"# serialize the ONNX model to a string\n",
					"blob = BlobClient.from_connection_string(conn_str=connection_string, container_name=\"synapsefs\", blob_name=\"models/hbmodel.onnx\")\n",
					"blob.upload_blob(hb_model.model.SerializeToString(),overwrite=True)"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"source": [
					"# now add to the spark context\n",
					"from pyspark import SparkFiles\n",
					"#abfss://synapsefs@negustdatalake.dfs.core.windows.net/models/\n",
					"#path = \"abfss://hummingbird@humingbird.dfs.core.windows.net/models/hbmodel.onnx\"\n",
					"path = \"abfss://synapsefs@negustdatalake.dfs.core.windows.net/models/hbmodel.onnx\"\n",
					"sc = raven_session.sparkContext\n",
					"sc.addFile(path)\n",
					"\n",
					"# make sure it's ok!\n",
					"model_check = SparkFiles.get(path)\n",
					"print(model_check)"
				],
				"execution_count": 17
			},
			{
				"cell_type": "markdown",
				"source": [
					"<img src=\"https://sqlchoice.blob.core.windows.net/sqlchoice/static/images/NDA.png\" alt=\"Surface Device\" width=\"10%\"/>\n",
					"\n",
					"# Project Raven\n",
					"## Use Spark SQL to call PREDICT + Optimizations\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Call PREDICT on Spark\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"withPredict = raven_session.sql(\"SELECT PREDICT(\\\"\"+ \"hbmodel.onnx\" + \"\\\",storeId,productCode,advertising,price,weekStarting,basePrice,wholeSaleCost,ratioAge60,collegeRatio,income,highIncome150Ratio,largeHH,minoritiesRatio,more1FullTimeEmployeeRatio,distanceNearestWarehouse,salesNearestWarehousesRatio,avgDistanceNearest5Supermarkets,salesNearest5StoresRatio) as prediction from combined\")\n",
					"\n",
					"#Show plan\n",
					"#withPredict.explain(True) "
				],
				"execution_count": 20
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Let's look at the predictions\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"withPredict.show(5)"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"source": [
					"from azureml.core import Workspace\n",
					"\n",
					"subscription_id = \"58f8824d-32b0-4825-9825-02fa6a801546\" #you should be owner or contributor\n",
					"resource_group = \"prlangadrg\" #you should be owner or contributor\n",
					"workspace_name = \"amlwsdemos\" #your workspace name\n",
					"\n",
					"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
					"\n",
					"print('Workspace name: ' + ws.name, \n",
					"      'Azure region: ' + ws.location, \n",
					"      'Subscription id: ' + ws.subscription_id, \n",
					"      'Resource group: ' + ws.resource_group, sep = '\\n')"
				],
				"attachments": null,
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"source": [
					"import mlflow\n",
					"import mlflow.onnx\n",
					"\n",
					"from mlflow.models.signature import infer_signature\n",
					"\n",
					"experiment_name = 'synapse_retail_model_hb'\n",
					"artifact_path = 'synapse_retail_model_hb_artifact'\n",
					"\n",
					"mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
					"mlflow.set_experiment(experiment_name)\n",
					"\n",
					"with mlflow.start_run() as run:\n",
					"    # Infer signature\n",
					"    input_sample = train_features.head(1).astype(np.float64)\n",
					"    output_sample = pd.DataFrame(columns=['quantity'], data=[1234.0]).astype(np.float64)\n",
					"    signature = infer_signature(input_sample, output_sample)\n",
					"\n",
					"    # Save the model to the outputs directory for capture\n",
					"    mlflow.onnx.log_model(hb_model.model, artifact_path, signature=signature)\n",
					"\n",
					"    # Register the model to AML model registry\n",
					"    mlflow.register_model('runs:/' + run.info.run_id + '/' + artifact_path, 'synapse_retail_model_hb')"
				],
				"execution_count": 19
			}
		]
	}
}