{
	"name": "Train model with retail data",
	"properties": {
		"folder": {
			"name": "Test notebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "analytics1",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7e416de3-c506-4776-8270-83fd73c6cc37/resourceGroups/demosynapserg/providers/Microsoft.Synapse/workspaces/wsazuresynapseanalytics/bigDataPools/analytics1",
				"name": "analytics1",
				"type": "Spark",
				"endpoint": "https://wsazuresynapseanalytics.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/analytics1",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 8,
				"memory": 56
			}
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import pip\n",
					"\n",
					"pip.main([\"install\", \"skl2onnx==1.7.0\"])\n",
					"pip.main([\"install\", \"mlflow==1.10.0\"])\n",
					"pip.main([\"install\", \"azureml-sdk==1.16.0\"])\n",
					"pip.main([\"install\", \"azureml-mlflow==1.16.0\"])"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import time\n",
					"import numpy as np\n",
					"import pandas as pd\n",
					"import onnxruntime as ort\n",
					"\n",
					"from datetime import datetime\n",
					"from timeit import default_timer as timer\n",
					"from sklearn.ensemble import RandomForestRegressor\n",
					"from sklearn.model_selection import train_test_split\n",
					"from sklearn.pipeline import Pipeline\n",
					"from sklearn.preprocessing import StandardScaler\n",
					"from sklearn.compose import ColumnTransformer\n",
					"from skl2onnx import convert_sklearn\n",
					"from skl2onnx.common.data_types import DoubleTensorType"
				],
				"attachments": null,
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Load data from Cosmos DB and convert to Pandas "
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"_rid"
							],
							"values": [
								"_ts"
							],
							"yLabel": "_ts",
							"xLabel": "_rid",
							"aggregation": "SUM",
							"aggByBackend": false
						},
						"aggData": "{\"_ts\":{\"sY0TAOBJNuEBAAAAAAAAAA==\":1598476485,\"sY0TAOBJNuECAAAAAAAAAA==\":1598476485,\"sY0TAOBJNuEDAAAAAAAAAA==\":1598476485,\"sY0TAOBJNuEEAAAAAAAAAA==\":1598476485,\"sY0TAOBJNuEFAAAAAAAAAA==\":1598476485}}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					},
					"collapsed": false
				},
				"source": [
					"df = spark.read\\\n",
					"    .format(\"cosmos.olap\")\\\n",
					"    .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
					"    .option(\"spark.cosmos.container\", \"RetailSales\")\\\n",
					"    .load()\\\n",
					"    .toPandas()\n",
					"\n",
					"display(df.head(5))"
				],
				"attachments": null,
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Define some featurizers: dictionary encoder and date featurizer"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"def dictionary_encode(df, column, dictionary):\n",
					"\n",
					"    def add_into_dict(value, dictionary):\n",
					"        count = 0\n",
					"        if len(dictionary) > 0:\n",
					"            count = max(dictionary.values())\n",
					"        count += 1\n",
					"        dictionary[value] = count\n",
					"        return count\n",
					"\n",
					"    df[column] = pd.DataFrame(df[column].values)[0].apply(lambda x: dictionary[x] if x in dictionary else add_into_dict(x, dictionary))\n",
					"\n",
					"def date_featurize(df,column):\n",
					"    df[column] = pd.DataFrame(df[column].values)[0].apply(lambda x: (int(datetime.strptime(x, '%m/%d/%Y').strftime('%m%d%Y'))))"
				],
				"attachments": null,
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Featurize the data"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"_rid"
							],
							"values": [
								"_ts"
							],
							"yLabel": "_ts",
							"xLabel": "_rid",
							"aggregation": "SUM",
							"aggByBackend": false
						},
						"aggData": "\"{\\\"_ts\\\":{\\\"Rnk0ALxVbBoBAAAAAAAAAA==\\\":1604468149,\\\"Rnk0ALxVbBoCAAAAAAAAAA==\\\":1604468149,\\\"Rnk0ALxVbBoDAAAAAAAAAA==\\\":1604468149,\\\"Rnk0ALxVbBoEAAAAAAAAAA==\\\":1604468149,\\\"Rnk0ALxVbBoFAAAAAAAAAA==\\\":1604468149}}\"",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					},
					"collapsed": false
				},
				"source": [
					"#dictionary = {}\n",
					"#dictionary_encode(df, 'ProductID', dictionary)\n",
					"date_featurize(df, 'Date')\n",
					"display(df.head(5))"
				],
				"attachments": null,
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Drop unnecessary columns\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"logQuantity"
							],
							"values": [
								"logQuantity"
							],
							"yLabel": "logQuantity",
							"xLabel": "logQuantity",
							"aggregation": "COUNT",
							"aggByBackend": false
						},
						"aggData": "{\"logQuantity\":{\"10.06967973\":1,\"10.60935351\":1,\"11.03823889\":1,\"9.189321005\":1,\"9.715711145\":1}}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					},
					"collapsed": false
				},
				"source": [
					"df = df.drop(columns=['_rid', '_ts', 'id', '_etag'])\n",
					"display(df.head(5))"
				],
				"attachments": null,
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Split training and testing data\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"train_df, test_df = train_test_split(df, test_size=0.2)\n",
					"\n",
					"train_features = pd.DataFrame(train_df.drop(['quantity'], axis = 1))\n",
					"train_labels = pd.DataFrame(train_df.iloc[:,train_df.columns.tolist().index('quantity')])\n",
					"\n",
					"test_features = pd.DataFrame(test_df.drop(['quantity'], axis = 1))\n",
					"test_labels = pd.DataFrame(test_df.iloc[:,test_df.columns.tolist().index('quantity')])"
				],
				"attachments": null,
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Write the training data to a Spark table. Later we can use the Spark table to kick off the AutoML UI"
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "sparksql"
					},
					"collapsed": false
				},
				"source": [
					"%%sql\r\n",
					"drop TABLE default.retail_training_data"
				],
				"attachments": null,
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": true
					}
				},
				"source": [
					"from pyspark.sql.functions import col\n",
					"\n",
					"spark_df = spark.createDataFrame(train_df)\n",
					"spark_df = spark_df.select([col(c).cast(\"double\") for c in spark_df.columns])\n",
					"spark_df.printSchema\n",
					"\n",
					"spark_df.write.mode(\"overwrite\").saveAsTable(\"default.retail_training_data\")"
				],
				"attachments": null,
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Write the testing data to a CSV file. Later we can load the CSV file into a SQL table for SQL scoring\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"csv = \"abfss://source@azdatalake2020.dfs.core.windows.net/retail_testing_data\"\n",
					"\n",
					"spark.createDataFrame(test_features)\\\n",
					"    .coalesce(1)\\\n",
					"    .write\\\n",
					"    .option(\"header\",\"true\")\\\n",
					"    .option(\"sep\",\",\")\\\n",
					"    .mode(\"overwrite\")\\\n",
					"    .csv(csv)"
				],
				"attachments": null,
				"execution_count": 39
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Train a Random Forest model\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"Quantity"
							],
							"values": [
								"Quantity"
							],
							"yLabel": "Quantity",
							"xLabel": "Quantity",
							"aggregation": "COUNT",
							"aggByBackend": false
						},
						"aggData": "\"{\\\"Quantity\\\":{\\\"23\\\":1,\\\"24\\\":1,\\\"25\\\":1,\\\"35\\\":1,\\\"59\\\":1}}\"",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					}
				},
				"source": [
					"scaler_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
					"preprocessor = ColumnTransformer(transformers=[('scaling', scaler_transformer, list(train_features.columns))])\n",
					"model = RandomForestRegressor(n_estimators=20, max_depth=9)\n",
					"pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
					"pipeline.fit(train_features, train_labels)"
				],
				"attachments": null,
				"execution_count": 40
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Convert the model to ONNX format\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"def input_types(features):\n",
					"    inputs = []\n",
					"    for columnName, columnType in zip(features.columns, features.dtypes):\n",
					"        inputs.append((str(columnName), DoubleTensorType([None, 1])))\n",
					"    return inputs\n",
					"\n",
					"onnx_model = convert_sklearn(pipeline, initial_types=input_types(train_features), final_types=[('Quantity', DoubleTensorType([None, 1]))])"
				],
				"attachments": null,
				"execution_count": 41
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Connect to the linked AML workspace\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"from azureml.core import Workspace\n",
					"\n",
					"subscription_id = \"58f8824d-32b0-4825-9825-02fa6a801546\" #you should be owner or contributor\n",
					"resource_group = \"prlangadrg\" #you should be owner or contributor\n",
					"workspace_name = \"amlwsdemos\" #your workspace name\n",
					"\n",
					"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
					"\n",
					"print('Workspace name: ' + ws.name, \n",
					"      'Azure region: ' + ws.location, \n",
					"      'Subscription id: ' + ws.subscription_id, \n",
					"      'Resource group: ' + ws.resource_group, sep = '\\n')"
				],
				"attachments": null,
				"execution_count": 42
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Register the model and its signature\n",
					""
				],
				"attachments": null
			},
			{
				"cell_type": "code",
				"source": [
					"import mlflow\n",
					"import mlflow.onnx\n",
					"from mlflow.models.signature import infer_signature\n",
					"\n",
					"demo_name = 'mechanics'\n",
					"spark_table_name = 'retail_data'\n",
					"experiment_name = demo_name + '-' + spark_table_name + '-' + '20201105231400'\n",
					"model_name = experiment_name + '-Best'\n",
					"artifact_path = 'outputs'\n",
					"\n",
					"mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
					"mlflow.set_experiment(experiment_name)\n",
					"\n",
					"with mlflow.start_run() as run:\n",
					"    # Infer signature\n",
					"    input_sample = train_features.head(1).astype(np.float64)\n",
					"    output_sample = pd.DataFrame(columns=['Quantity'], data=[1234.0]).astype(np.float64)\n",
					"    signature = infer_signature(input_sample, output_sample)\n",
					"\n",
					"    # Save the model to the outputs directory for capture\n",
					"    mlflow.onnx.log_model(onnx_model, artifact_path, signature=signature)\n",
					"\n",
					"    # Register the model to AML model registry\n",
					"    mlflow.register_model('runs:/' + run.info.run_id + '/' + artifact_path, model_name)"
				],
				"attachments": null,
				"execution_count": 46
			}
		]
	}
}